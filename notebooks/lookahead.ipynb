{
    "cells": [{
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now we're going to try to make our agent survive longer by using the lookahead functionnality, *i.e.* using a **tactical horizon** greater than 1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "from plan4grid import Launcher"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "env_name = \"educ_case14_storage\"\n",
                "scenario_id = 1\n",
                "tactical_horizon = 3\n",
                "launcher = Launcher(env_name=env_name, scenario_id=scenario_id, tactical_horizon=tactical_horizon)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here we use a **tactical horizon** of 3 time steps. This means that our agent will look ahead 3 time steps and plan actions accordingly. He will then execute the first action of the plan and then look ahead again. The agent will repeat this process until here reach the goal or until he reach a dead end.\n",
                "\n",
                "*Note: becareful, the higher the tactical horizon, the longer it will take to compute the plan, i.e. solve the associated optimization problem. And it's clearly not linear.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [{
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Running the agent on scenario 1 for 96 steps of 15 minutes.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Steps:  80%|████████  | 77/96 [00:29<00:07,  2.62it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Total reward: 1183.8366765975952\n"
                    ]
                },
                {
                    "data": {

                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Logs are available in D:\\AIPlan4Grid\\.venv\\Lib\\site-packages\\log\\AIPlan4GridAgent.log\n",
                        "Results are available in D:\\AIPlan4Grid\\.venv\\Lib\\site-packages\\results\\AIPlan4GridAgent\n"
                    ]
                }
            ],
            "source": [
                "launcher.launch(save=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Let's see how it goes:\n",
                "\n",
                "![log of the agent](./assets/agent_3.png)\n",
                "\n",
                "Here we see that the agent, who has a tactical horizon of 3 timesteps, detects that there will be a congestion in 3 timesteps, so he launches the problem solving process.\n",
                "\n",
                "![log of the optimizer](./assets/optimizer_3.png)\n",
                "\n",
                "We can see that the optimizer has found a solution to the problem but the \"*advance_step*\" action means doing nothing, so the above plan consists of doing nothing for 2 time steps and then starts acting on the storage unit.\n",
                "\n",
                "This will solve the congestion problem but unfortunately it will reappear again and again and finally the agent will not survive for the same reasons as in the previous notebook. But we can observe that compared to the agent which acts on a single time step, the fact of having used a tactical horizon of 3 time steps has enabled him to survive longer --> 5 minutes more."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "When you read this notebook you'll probably wonder, and rightly so, why the optimizer doesn't charge the batteries prophylactically so that they can be discharged later to stem congestion, rather than doing nothing? Well, the fact is that in this case the structure of the powergrid and the initial conditions mean that the flow on the line in question is already very close to the limit and recharging the battery would create congestion... so it's a case of the snake biting its own tail in this situation. One potential solution is to impose constraints on the batteries, preventing them from going below (or above) their initial state of charge (SOC) at the end of the schedule. While this might address the issue, it may not be practically feasible. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Finally, you can see the actions took by the agent using the [`Grid2Viz`](https://github.com/rte-france/grid2viz) tool.\n",
                "*Note that to do this, you must have run the program with the save option set to true*.\n",
                "\n",
                "Just run the following command in a terminal:\n",
                "\n",
                "```bash\n",
                "cd scripts\n",
                "chmod +x visualize.sh\n",
                "./visualize.sh <results_path> <g2op_env_path>\n",
                "```\n",
                "\n",
                "Where `<results_path>` is the path to the results directory and `<g2op_env_path>` is the path to the environment data directory stored by [Grid2Op](https://github.com/rte-france/Grid2Op) in a folder called \"`data_grid2op`\"."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Here's a preview of what the web app launched by the script looks like:\n",
                "\n",
                "Scenario overview:\n",
                "\n",
                "![scenario overview](./assets/grid2viz_1.png)\n",
                "\n",
                "Agent study:\n",
                "\n",
                "![agent study](./assets/grid2viz_2.png)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}